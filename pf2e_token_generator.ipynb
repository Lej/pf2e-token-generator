{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lej/pf2e-token-generator/blob/main/pf2e_token_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "RalN2mNzFS_3",
        "outputId": "ff74288c-5b4e-4774-bfdb-a0134f1d9520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul  6 08:56:27 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Running Step 1: clone_pf2e\n",
            "/content\n",
            "Cloning into 'pf2e'...\n",
            "remote: Enumerating objects: 1, done.\u001b[K\n",
            "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 1 (delta 0), reused 1 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1/1), done.\n",
            "/content/pf2e\n",
            "remote: Enumerating objects: 556, done.\u001b[K\n",
            "remote: Counting objects: 100% (556/556), done.\u001b[K\n",
            "remote: Compressing objects: 100% (513/513), done.\u001b[K\n",
            "remote: Total 556 (delta 0), reused 342 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (556/556), 893.23 KiB | 2.54 MiB/s, done.\n",
            "^C\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Expected exit code 0 but got -2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ad62d819b18>\u001b[0m in \u001b[0;36m<cell line: 281>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_gpu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_pf2e\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone_pf2e_token_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6ad62d819b18>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(step, callback)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prev_step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running Step {step}: {step_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prev_step\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6ad62d819b18>\u001b[0m in \u001b[0;36mclone_pf2e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0massert_exit_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'git checkout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0massert_exit_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m   \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-6ad62d819b18>\u001b[0m in \u001b[0;36massert_exit_code\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected exit code 0 but got {exit_code}: {message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected exit code 0 but got {exit_code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Expected exit code 0 but got -2"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import io\n",
        "import inspect\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from IPython import get_ipython\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "\n",
        "def get_exit_code():\n",
        "  return get_ipython().__dict__[\"user_ns\"][\"_exit_code\"]\n",
        "\n",
        "def is_gpu():\n",
        "  !nvidia-smi\n",
        "  return get_exit_code() == 0\n",
        "\n",
        "def assert_exit_code(message = None):\n",
        "  exit_code = get_exit_code()\n",
        "  if exit_code != 0:\n",
        "    if (message != None):\n",
        "      raise Exception(f\"Expected exit code 0 but got {exit_code}: {message}\")\n",
        "    else:\n",
        "      raise Exception(f\"Expected exit code 0 but got {exit_code}\")\n",
        "\n",
        "def step(step, callback):\n",
        "  step_name = callback.__name__\n",
        "  if (step > state[\"prev_step\"]):\n",
        "    print(f'Running Step {step}: {step_name}')\n",
        "    result = callback()\n",
        "    state[\"prev_step\"] = step\n",
        "    return result\n",
        "  else:\n",
        "    print(f\"Skipping Step {step}: {step_name}\")\n",
        "    return None\n",
        "\n",
        "def install_pipe():\n",
        "  #!pip install diffusers==0.11.1\n",
        "  !pip install diffusers\n",
        "  assert_exit_code()\n",
        "  !pip install transformers scipy ftfy accelerate\n",
        "  assert_exit_code()\n",
        "  !pip install cairosvg\n",
        "  assert_exit_code()\n",
        "\n",
        "def create_pipe():\n",
        "  from diffusers import StableDiffusionPipeline\n",
        "  import torch\n",
        "  pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
        "  return pipe.to(\"cuda\")\n",
        "\n",
        "def clone_pf2e():\n",
        "  %cd /content\n",
        "  !rm -rf ./pf2e\n",
        "  assert_exit_code()\n",
        "  !git clone --no-checkout --depth=1 --filter=tree:0 https://github.com/foundryvtt/pf2e.git\n",
        "  assert_exit_code()\n",
        "  %cd ./pf2e\n",
        "  !git sparse-checkout set --no-cone packs static\n",
        "  assert_exit_code()\n",
        "  !git checkout\n",
        "  assert_exit_code()\n",
        "  %cd /content\n",
        "\n",
        "def clone_pf2e_token_generator():\n",
        "  %cd /content\n",
        "  !rm -rf ./pf2e-token-generator\n",
        "  assert_exit_code()\n",
        "  !git clone https://github.com/Lej/pf2e-token-generator.git\n",
        "  assert_exit_code()\n",
        "  %cd /content\n",
        "\n",
        "def get_or_default(root, keys, default):\n",
        "  current = root\n",
        "  for key in keys:\n",
        "    next = current.get(key)\n",
        "    if next == None:\n",
        "      return default\n",
        "    else:\n",
        "      current = next\n",
        "  return current\n",
        "\n",
        "def get_prompt(npc):\n",
        "  name = get_or_default(npc, [\"name\"], \"\")\n",
        "  traits = get_or_default(npc, [\"system\", \"traits\", \"value\"], [])\n",
        "  traitsText = \" \".join(traits)\n",
        "  blurb = get_or_default(npc, [\"system\", \"details\", \"blurb\"], \"\")\n",
        "  spellcasting = get_or_default(npc, [\"system\", \"spellcasting\"], {})\n",
        "  spellcastingText = \" \".join(spellcasting.keys())\n",
        "  #artist = \"Wayne Reynolds\"\n",
        "  artist = \"Greg Rutkowski\"\n",
        "  prompt = f\"Fantasy art {name} {traitsText} {blurb} {spellcastingText} in the style of {artist}\"\n",
        "  regexes = [\n",
        "    \"\\([^\\)]*\\d[^\\)]*\\)\", # (7-8), (Tier 5-6), (G4), (PFS 1-24, Staff)\n",
        "    \"\\(BB|SOT|AoE|PFS\\)\", # (BB), (SOT), (AoE), (PFS)\n",
        "    \"\\(|\\)\", # (, )\n",
        "    \"\\s+\" # multiple whitespace\n",
        "  ]\n",
        "  for regex in regexes:\n",
        "    prompt = re.sub(regex, \" \", prompt, flags=re.IGNORECASE)\n",
        "  return prompt\n",
        "\n",
        "def timestamp():\n",
        "    return int((datetime.utcnow() - datetime(1970, 1, 1)).total_seconds() * 1000)\n",
        "\n",
        "def create_prompts():\n",
        "  prompts = []\n",
        "  with open(\"/content/pf2e/static/system.json\") as f:\n",
        "    system = json.load(f)\n",
        "  for pack in system[\"packs\"]:\n",
        "    print(f\"Pack {pack}\")\n",
        "    packName = pack[\"name\"]\n",
        "    packPath = pack[\"path\"]\n",
        "    globPath = f\"/content/pf2e/{packPath}/*.json\"\n",
        "    for path in glob.glob(globPath, recursive=False):\n",
        "      with open(path) as f:\n",
        "        doc = json.load(f)\n",
        "      if (isinstance(doc, dict) and doc.get(\"type\") == \"npc\"):\n",
        "        id = doc.get(\"_id\")\n",
        "        #compendium = re.search(r'.*/([^/]+?)/[^/]+', path).group(1)\n",
        "        prompt = {}\n",
        "        prompt[\"id\"] = id\n",
        "        prompt[\"compendium\"] = packName\n",
        "        prompt[\"name\"] = doc.get(\"name\")\n",
        "        prompt[\"prompt\"] = get_prompt(doc)\n",
        "        #prompt[\"timestamp\"] = timestamp()\n",
        "        prompt[\"seed\"] = 1024\n",
        "        prompts.append(prompt)\n",
        "  config = {}\n",
        "  config[\"prompts\"] = prompts\n",
        "  with open(f\"/content/prompts.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(config, indent=4))\n",
        "  print(f\"Created {len(prompts)} prompts.\")\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def generate_border():\n",
        "  !apt install librsvg2-bin\n",
        "  !mkdir -p /content/pf2e-token-generator/images\n",
        "  !rsvg-convert -w 512 -h 512 /content/pf2e-token-generator/resources/border.svg -o /content/pf2e-token-generator/images/border.png\n",
        "\n",
        "def generate_images():\n",
        "  if (not state[\"is_gpu\"]):\n",
        "    print(f\"Skipping image generation: Not GPU\")\n",
        "    return\n",
        "  border = Image.open(\"/content/pf2e-token-generator/images/border.png\").convert('RGBA')\n",
        "  mask = Image.open(\"/content/pf2e-token-generator/resources/mask.png\").convert('RGBA')\n",
        "  with open(\"/content/prompts.json\") as f:\n",
        "    new = json.load(f)\n",
        "\n",
        "  staged = 0;\n",
        "  current = 1\n",
        "  total = len(new[\"prompts\"])\n",
        "  message = \"\"\n",
        "  for prompt in new[\"prompts\"]:\n",
        "    print(f\"Prompt {current}/{total}\")\n",
        "    current = current + 1\n",
        "    with open(\"/content/pf2e-token-generator/generated.json\") as f:\n",
        "      old = json.load(f)\n",
        "    id = prompt[\"id\"]\n",
        "    print(f\"{id}\")\n",
        "    oldPrompt = old[id][\"prompt\"] if id in old and \"prompt\" in old[id] else \"\"\n",
        "    newPrompt = prompt[\"prompt\"]\n",
        "    if (oldPrompt == newPrompt):\n",
        "      print(f\"Skip\")\n",
        "      continue\n",
        "    print(f\"Generating: {newPrompt}\")\n",
        "    seed = prompt[\"seed\"]\n",
        "    used_seed = seed - 1\n",
        "    ok = False\n",
        "    while (not ok):\n",
        "      used_seed = used_seed + 1\n",
        "      #prompt[\"used_seed\"] = used_seed\n",
        "      print(prompt)\n",
        "      import torch\n",
        "      generator = torch.Generator(\"cuda\").manual_seed(used_seed)\n",
        "      result = state[\"pipe\"](newPrompt, generator=generator)\n",
        "      ok = not result.nsfw_content_detected[0]\n",
        "      if (used_seed > seed + 10):\n",
        "        raise Exception(f\"Failed to generated SFW image\")\n",
        "    staged = staged + 1\n",
        "    generated = {}\n",
        "    generated[\"id\"] = prompt[\"id\"]\n",
        "    generated[\"prompt\"] = newPrompt\n",
        "    generated[\"timestamp\"] = timestamp()\n",
        "    generated[\"seed\"] = used_seed\n",
        "    old[id] = generated\n",
        "    actor = result.images[0]\n",
        "    #else:\n",
        "      #actor = Image.open(\"/content/pf2e-token-generator/resources/todo.png\").convert('RGBA')\n",
        "    token = Image.composite(actor, mask, mask)\n",
        "    token.paste(border, mask=border)\n",
        "    #compendium = prompt[\"compendium\"]\n",
        "    !mkdir -p /content/pf2e-token-generator/images/$id\n",
        "    actor.resize((256, 256)).save(f\"/content/pf2e-token-generator/images/{id}/actor.webp\", format=\"webp\")\n",
        "    token.resize((256, 256)).save(f\"/content/pf2e-token-generator/images/{id}/token.webp\", format=\"webp\")\n",
        "    with open(\"/content/pf2e-token-generator/generated.json\", \"w\") as outfile:\n",
        "      outfile.write(json.dumps(old, indent=4))\n",
        "    #with open(\"/content/pf2e-token-generator/art-mapping.json\") as f:\n",
        "    #  art_mapping = json.load(f)\n",
        "    #if (compendium not in art_mapping):\n",
        "    #  art_mapping[compendium] = {}\n",
        "    #if (id not in art_mapping[compendium]):\n",
        "    #  art_mapping[compendium][id] = {}\n",
        "    #art_mapping[compendium][id][\"actor\"] = f\"modules/pf2e-ai-token-placeholders/images/{compendium}/{id}/actor.webp\"\n",
        "    #art_mapping[compendium][id][\"token\"] = f\"modules/pf2e-ai-token-placeholders/images/{compendium}/{id}/token.webp\"\n",
        "    #with open(\"/content/pf2e-token-generator/art-mapping.json\", \"w\") as outfile:\n",
        "    #  outfile.write(json.dumps(art_mapping, indent=4))\n",
        "\n",
        "    # git\n",
        "\n",
        "    ts = generated[\"timestamp\"]\n",
        "    message = message + f\"Generated {id} {ts}\\n\"\n",
        "    if (staged % 20 == 0):\n",
        "      !git add --all\n",
        "      assert_exit_code()\n",
        "      !git commit -m \"$message\"\n",
        "      assert_exit_code()\n",
        "      !git push origin main\n",
        "      assert_exit_code()\n",
        "      staged = 0\n",
        "      message = \"\"\n",
        "    #if (not state[\"is_gpu\"]):\n",
        "    #  break\n",
        "\n",
        "def generate_art_mapping():\n",
        "  art_mapping = {}\n",
        "  with open(\"/content/prompts.json\") as f:\n",
        "    new = json.load(f)\n",
        "  for prompt in new[\"prompts\"]:\n",
        "    compendium = prompt[\"compendium\"]\n",
        "    if (compendium not in art_mapping):\n",
        "      art_mapping[compendium] = {}\n",
        "    id = prompt[\"id\"]\n",
        "    if (id not in art_mapping[compendium]):\n",
        "      art_mapping[compendium][id] = {}\n",
        "    art_mapping[compendium][id][\"actor\"] = f\"modules/pf2e-ai-token-placeholders/images/{id}/actor.webp\"\n",
        "    art_mapping[compendium][id][\"token\"] = f\"modules/pf2e-ai-token-placeholders/images/{id}/token.webp\"\n",
        "  with open(\"/content/pf2e-token-generator/art-mapping.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(art_mapping, indent=4))\n",
        "  !git add --all\n",
        "  assert_exit_code()\n",
        "  ts = timestamp()\n",
        "  message = f\"Update art mapping {ts}\"\n",
        "  !git commit -m \"$message\"\n",
        "  assert_exit_code()\n",
        "  !git push origin main\n",
        "  assert_exit_code()\n",
        "\n",
        "def git_setup():\n",
        "  with open(\"/content/drive/MyDrive/pf2e-token-generator/github-pat.json\") as f:\n",
        "    credentials = json.load(f)\n",
        "  name = credentials[\"name\"]\n",
        "  email = credentials[\"email\"]\n",
        "  username = credentials[\"username\"]\n",
        "  pat = credentials[\"pat\"]\n",
        "  %cd /content/pf2e-token-generator/\n",
        "  !git config --global user.email $email\n",
        "  !git config --global user.name \"{name}\"\n",
        "  !git remote set-url origin https://$username:$pat@github.com/$username/pf2e-token-generator.git\n",
        "\n",
        "# Run\n",
        "if not \"state\" in globals():\n",
        "  state = {\n",
        "      \"prev_step\": 0\n",
        "  }\n",
        "\n",
        "#!nvidia-smi\n",
        "#assert_exit_code(\"Is runtime type set to GPU?\")\n",
        "state[\"is_gpu\"] = is_gpu()\n",
        "\n",
        "step(1, clone_pf2e)\n",
        "step(2, clone_pf2e_token_generator)\n",
        "step(3, create_prompts)\n",
        "if (state[\"is_gpu\"]):\n",
        "  step(4, install_pipe)\n",
        "  state[\"pipe\"] = step(5, create_pipe) or state[\"pipe\"]\n",
        "step(6, generate_border)\n",
        "step(7, git_setup)\n",
        "step(8, generate_images)\n",
        "step(9, generate_art_mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pXe1D2Ol6ewD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YfzEFpaUCNEQ"
      },
      "outputs": [],
      "source": [
        "state[\"prev_step\"] = 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pf2e-token-generator/\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZB4F2z5p2FA",
        "outputId": "e432a8a6-e66d-4512-b90a-c27fd57a2134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pf2e-token-generator\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 324 bytes | 2.00 KiB/s, done.\n",
            "From https://github.com/Lej/pf2e-token-generator\n",
            "   32bbae1f..e681fc92  main       -> origin/main\n",
            "Updating 32bbae1f..e681fc92\n",
            "Fast-forward\n",
            " generated.json | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_prompts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT_4oPIIqFP-",
        "outputId": "f911dbed-452f-413c-cd4b-66b1aea1f423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 4182 prompts.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sEP5NnmJAas8AbAX_v5K-bVrUhte3FY3",
      "authorship_tag": "ABX9TyN5tQi9uWQ7MVAQiET9YYJl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}